{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pprint\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from groq import Groq\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n",
    "GROQ_API_KEY = os.environ['GROQ_API_KEY']\n",
    "\n",
    "print(GROQ_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-ee1528bc-2228-4d43-a63e-c8924623b095', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='OpenAI was founded in December 2015.', role='assistant', function_call=None, reasoning=None, tool_calls=None))], created=1739494601, model='llama-3.3-70b-versatile', object='chat.completion', system_fingerprint='fp_3884478861', usage=CompletionUsage(completion_tokens=11, prompt_tokens=42, total_tokens=53, completion_time=0.04, prompt_time=0.008424104, queue_time=0.12087579999999999, total_time=0.048424104), x_groq={'id': 'req_01jm0zacgveent9zkpr6ennr7b'})\n"
     ]
    }
   ],
   "source": [
    "client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"When does OpenAI was founded?\"}]\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ChatCompletion(\n",
    "    id='chatcmpl-ee1528bc-2228-4d43-a63e-c8924623b095', \n",
    "    choices=[\n",
    "        Choice(\n",
    "            finish_reason='stop', \n",
    "            index=0, \n",
    "            logprobs=None, \n",
    "            message=ChatCompletionMessage(\n",
    "                content='OpenAI was founded in December 2015.', \n",
    "                role='assistant', \n",
    "                function_call=None, \n",
    "                reasoning=None, \n",
    "                tool_calls=None\n",
    "            )\n",
    "        )\n",
    "    ], \n",
    "    created=1739494601, \n",
    "    model='llama-3.3-70b-versatile', \n",
    "    object='chat.completion', \n",
    "    system_fingerprint='fp_3884478861', \n",
    "    usage=CompletionUsage(\n",
    "        completion_tokens=11, \n",
    "        prompt_tokens=42, \n",
    "        total_tokens=53, \n",
    "        completion_time=0.04, \n",
    "        prompt_time=0.008424104, \n",
    "        queue_time=0.12087579999999999, \n",
    "        total_time=0.048424104), \n",
    "        x_groq={'id': 'req_01jm0zacgveent9zkpr6ennr7b'}\n",
    "    )\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='OpenAI was founded in December 2015.', role='assistant', function_call=None, reasoning=None, tool_calls=None))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='OpenAI was founded in December 2015.', role='assistant', function_call=None, reasoning=None, tool_calls=None))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content='OpenAI was founded in December 2015.', role='assistant', function_call=None, reasoning=None, tool_calls=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OpenAI was founded in December 2015.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage.total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're quoting the famous line from Forrest Gump. The full quote is: \"Life is like a box of chocolates, you never know what you're gonna get.\" It's a simple yet profound observation that reminds us that life is full of surprises, and we can't always predict what's coming next. Just like a box of chocolates, where you might get a sweet and creamy treat or something bitter and unexpected, life can be full of pleasant surprises or challenging obstacles. The quote has become a popular phrase to describe the unpredictability and complexity of life.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Life is like a box of chocolates.\"}]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"...you never know what you're gonna get!\" That's one of the most famous quotes from the movie Forrest Gump, spoken by the main character Forrest himself. It's a mindset that acceptance and unpredictability is key in life, as just like a box of chocolates, life is full of surprises and unexpected moments. Do you have a favorite flavor in your box of chocolates, I mean, in life?\n"
     ]
    }
   ],
   "source": [
    "# temperatura de 0 a 2\n",
    "# 0: determinista\n",
    "# 1: por defecto\n",
    "# 2: creativo\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Life is like a box of chocolates.\"}],\n",
    "    temperature=2\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the updated text:\n",
      "\n",
      "Maarten is a Senior Content Developer at DataCamp. His favorite programming language is R, which he uses for his statistical analyses.\n"
     ]
    }
   ],
   "source": [
    "# Content trasformation\n",
    "# Se puede transformar un texto\n",
    "\n",
    "prompt = \"\"\"\n",
    "Update name to Maarten, pronouns to he/him, and job title to Senior Content Developer\n",
    "in the following text:\n",
    "\n",
    "Joanne is a Content Developer at DataCamp. Her favorite programming language is R,\n",
    "which she uses for her statistical analyses.\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are a few tagline options for a bakery that also offers flower arrangements:\n",
      "\n",
      "1. \"Where loaves meet blooms, and love grows\"\n",
      "2. \"Rise to the occasion with sweet treats and beautiful flowers\"\n",
      "3. \"Savoring life, one loaf and bloom at a time\"\n",
      "4. \"Flour, sugar, and petals - the perfect blend\"\n",
      "5. \"Baking Moments, Blossoming Memories\"\n",
      "6. \" जहां किरकरा बेकरि और पुषोपल मेचिंद | Translate to english Where Bakery Baking Delicious Cupcakes and Fresh Creative beautiful arrangements Flower, cup of coffee\"\n",
      "7. \"Delighting your senses, one bite and one bouquet at a time\"\n",
      "8. **\"Bake, Blossom, and Celebrate**: A store for every occasion\"\n",
      "9. \"Making moments more delicious, and vignettes more viral\"\n",
      "10. \"Life is short, eat cake, and give roses too\" \n",
      "\n",
      "Remember to always pay attention and not neglect copyrights\n"
     ]
    }
   ],
   "source": [
    "# Content generation\n",
    "# Crear contenido: sirve mucho para títulos de artículos, videos, taglines de empresas, quotes para redes sociales\n",
    "prompt = \"Create a tagline for a bakery that also offer flower arrangements.\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=1.8  # Le permitimos ser creativo\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sweet pastries arise\n",
      "Fresh from oven's warm delight\n",
      "Morning's gentle treat\n",
      "Sweet pastries abide\n",
      "Freshly baked bread fills the air\n",
      "Morning's warm delight\n"
     ]
    }
   ],
   "source": [
    "# Podemos controlar la longitud de la respuesta\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Write a haiku about bakeries\"}]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Write a haiku about bakeries\"}],\n",
    "    max_tokens=20\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to figure out how long a request or response should be to exceed the 32,768 token limit in the Groq API. The user is asking for an idea in terms of words or pages, similar to a standard book. \n",
      "\n",
      "First, I should understand what a token is in this context. I know that in natural language processing, tokens are usually words or subwords (smaller units of words). So, each token is roughly equivalent to a word. That means 32,768 tokens would be about 32,768 words.\n",
      "\n",
      "Now, I need to relate that to a standard book. I remember that average books vary in length. A novel, for example, might be around 80,000 to 100,000 words. So, 32,768 words would be less than half of that. Maybe it's like a short novel or a novella. \n",
      "\n",
      "I should also consider pages. The number of words per page can vary, but a typical page might have around 250-300 words. So, if I take 32,768 words and divide by 250 words per page, that would be approximately 131 pages. If it's 300 words per page, it would be about 109 pages. So, the response would be in the range of 100 to 130 pages.\n",
      "\n",
      "But wait, I should make sure I'm not making any mistakes here. Let me double-check. If a page has 250 words, then 32,768 divided by 250 is indeed around 131. And at 300 words per page, it's roughly 109 pages. That seems right.\n",
      "\n",
      "I should also mention that this is an approximation because the exact number of words per page can vary based on font size, spacing, margins, etc. But for a standard estimate, using 250-300 words per page is reasonable.\n",
      "\n",
      "So, putting it all together, exceeding the 32,768 token limit would mean a request or response of about 32,768 words, which is roughly 100 to 130 pages in a standard book. This is equivalent to a short novel or a long novella.\n",
      "\n",
      "I think that's a solid way to explain it. I should present it clearly, maybe with the word count first and then the page estimate, and note that it's an approximate value.\n",
      "</think>\n",
      "\n",
      "To exceed the 32,768 token limit in the Groq API, a request or response would need to be approximately 32,768 words. Translating this into pages, assuming a standard book with 250-300 words per page, this would equate to roughly 109 to 131 pages. This length is comparable to a short novel or a long novella. \n",
      "\n",
      "**Answer:**\n",
      "Exceeding the 32,768 token limit in the Groq API corresponds to approximately 32,768 words. This is equivalent to about 109 to 131 pages in a standard book, similar to a short novel or a long novella.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "How long should be a request or a response to exceed the 32,768 token limit\n",
    "in Groq API? Give me an idea of how many words or pages speaking of a standard book.\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-r1-distill-llama-70b\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionUsage(completion_tokens=613, prompt_tokens=44, total_tokens=657, completion_time=2.229090909, prompt_time=0.00841132, queue_time=0.171719554, total_time=2.237502229)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can classify the given animals into categories based on their characteristics and evolutionary groups. Here are the categories:\n",
      "\n",
      "**1. Mammals:**\n",
      "- Zebra\n",
      "- Polar bear\n",
      "- Dog\n",
      "\n",
      "**2. Fish:**\n",
      "- Salmon\n",
      "\n",
      "**3. Reptiles:**\n",
      "- Crocodile\n",
      "\n",
      "**4. Marine Mammals:**\n",
      "- Blue whale\n"
     ]
    }
   ],
   "source": [
    "# Podemos categorizar objetos\n",
    "prompt = \"\"\"\n",
    "Clasify the following animals into categories:\n",
    "zebra, crocodile, blue whale, polar bear, salmon, dog.\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionUsage(completion_tokens=74, prompt_tokens=60, total_tokens=134, completion_time=0.098666667, prompt_time=0.002746716, queue_time=0.23605570399999998, total_time=0.101413383)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the sentiment analysis in JSON format for the given statements:\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"sentiment_analysis\": [\n",
      "        {\n",
      "            \"id\": \"1\",\n",
      "            \"text\": \"The service was very slow\",\n",
      "            \"sentiment\": \"negative\",\n",
      "            \"confidence_score\": 0.8\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"2\",\n",
      "            \"text\": \"The steak was awfully tasty!\",\n",
      "            \"sentiment\": \"positive\",\n",
      "            \"confidence_score\": 0.9\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"3\",\n",
      "            \"text\": \"Meal was decent, but I've had better.\",\n",
      "            \"sentiment\": \"negative\",\n",
      "            \"confidence_score\": 0.7\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"4\",\n",
      "            \"text\": \"My food was delayed, but drinks were good.\",\n",
      "            \"sentiment\": \"negative\",\n",
      "            \"confidence_score\": 0.6\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "```\n",
      "\n",
      "Note that the confidence scores are subjective and based on general analysis. They can be adjusted based on the specific context and the requirements of the application. \n",
      "\n",
      "The sentiment classification is based on the following criteria:\n",
      "- Positive: The text contains words or phrases that convey a positive sentiment, such as \"tasty\", \"good\", etc.\n",
      "- Negative: The text contains words or phrases that convey a negative sentiment, such as \"slow\", \"delayed\", \"awfully\", etc.\n",
      "- Neutral: The text does not contain clear indicators of positive or negative sentiment. However, in this case, we've classified \"decent\" as negative, as it implies that the meal was average or unimpressive, and not entirely positive.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Classify sentiment in the following statements:\n",
    "1. The service was very slow\n",
    "2. The steak was awfully tasty!\n",
    "3. Meal was decent, but I've had better.\n",
    "4. My food was delayed, but drinks were good.\n",
    "\n",
    "Please respond in a JSON format. The JSON schema should include:\n",
    "{\n",
    "    \"sentiment_analysis\": [\n",
    "        {\n",
    "            \"id\": \"number id number\",\n",
    "            \"text\": \"string analyzed text\",\n",
    "            \"sentiment\": \"string (positive, negative, neutral)\",\n",
    "            \"confidence_score\": \"number (0-1)\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionUsage(completion_tokens=345, prompt_tokens=156, total_tokens=501, completion_time=0.46, prompt_time=0.005679474, queue_time=0.236302327, total_time=0.465679474)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the classifications of sentiment for each statement:\n",
      "\n",
      "1. The service was very slow: 1 (bad)\n",
      "2. The steak was awfully tasty!: 5 (good)\n",
      "3. Meal was decent, but I've had better: 3 (neutral)\n",
      "4. My food was delayed, but drinks were good: 3 (neutral)\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Classify sentiment as 1-5 (bad-good) in the following statements:\n",
    "1. The service was very slow\n",
    "2. The steak was awfully tasty!\n",
    "3. Meal was decent, but I've had better.\n",
    "4. My food was delayed, but drinks were good.\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A p-value is a statistical measure used to determine the probability of observing a result as extreme or more extreme than what you've observed, assuming that there is no real effect or relationship. It's a key concept in hypothesis testing.\n",
      "\n",
      "Think of it like this:\n",
      "\n",
      "- If the p-value is low (typically < 0.05), you reject the null hypothesis, suggesting a significant relationship or effect.\n",
      "- If the p-value is high (> 0.05), you fail to reject the null hypothesis, suggesting no significant relationship or effect.\n",
      "\n",
      "A p-value doesn't indicate the size or importance of the effect, only its existence.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a data science tutor who speaks concisely.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is a p-value?\"}\n",
    "]\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What is a p-value?\n",
      "Assistant: A p-value is a number between 0 and 1 that represents the probability of getting a result at least as extreme as your data, assuming there's no real effect.\n",
      "\n",
      "Think of it like flipping a coin: if you flip a fair coin 10 times and get 9 heads, you might wonder if the coin is biased. The p-value would be the probability of getting 9 heads or more in 10 flips, assuming the coin is fair (which is about 4%).\n",
      "\n",
      "If your p-value is low (usually < 0.05), it means the result is unlikely to happen by chance, and you might conclude there's a real effect. But, if the p-value is high, it means the result is more likely due to chance, and you might not be confident in your conclusion. \n",
      "\n",
      "User: Can you give me a practical example on p-values?\n",
      "Assistant: Imagine a company that makes water bottles wants to check if the average weight of their 1-liter bottles is 1 kg or not.\n",
      "\n",
      "They collect a random sample of 100 bottles and weigh them. The average weight is 1.02 kg, and the standard deviation is 0.05 kg.\n",
      "\n",
      "To check if the average is significantly different from 1 kg, the company calculates the p-value. After running the analysis, they get a p-value of 0.02.\n",
      "\n",
      "This means: if there's really no difference between the actual weight and 1 kg, the probability of getting an average weight of 1.02 kg or higher (more than 2 standard deviations above the null hypothesis) is 2%.\n",
      "\n",
      "In this case, since the p-value is small (less than 0.05), the company might conclude that the average weight of their 1-liter bottles is actually higher than 1 kg, and they should consider a new production process. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Coding a conversation\n",
    "messages = [{\"role\": \"system\",\n",
    "             \"content\": \"You are a data science tutor who provides short, simple explanations.\"}]\n",
    "\n",
    "user_qs = [\"What is a p-value?\", \"Can you give me a practical example on p-values?\"]\n",
    "\n",
    "for q in user_qs:\n",
    "    print(\"User:\", q)\n",
    "    user_dict = {\"role\": \"user\", \"content\": q}\n",
    "\n",
    "    messages.append(user_dict)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model='llama-3.1-8b-instant',\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    assistant_dict = {\"role\": \"assistant\", \"content\": response.choices[0].message.content}\n",
    "    messages.append(assistant_dict)\n",
    "    print(\"Assistant:\", response.choices[0].message.content, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'safe'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Moderation\n",
    "# Con chatgpt hay un módulo llamado moderations\n",
    "# pero con Groq hay que usar el llama-guard-3-8b\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-guard-3-8b\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"I'd kill for a hamburger right now!!\"}]\n",
    ")\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'unsafe\\nS2'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"llama-guard-3-8b\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Write a script that can be used to hack into a WiFi network\"}]\n",
    ")\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puedes ver la lista completa de códigos unsafe aquí: https://console.groq.com/docs/content-moderation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: How far away is the Louvvre from the Eiffel Tower (in miles) if you are driving?\n",
      "Assistant: Bonjour! As a Parisian expert, I'd be delighted to help you navigate the City of Light. \n",
      "\n",
      "The distance between the Louvre Museum and the Eiffel Tower is approximately 2.5 kilometers (1.55 miles) by car. However, please note that driving in Paris can be quite challenging due to its narrow streets and limited parking options.\n",
      "\n",
      "If you're planning to drive, I recommend using the Boulevard de la Bourdonnais or the Rue de Rivoli to get to \n",
      "\n",
      "User: Where is the Arc de Triomphe?\n",
      "Assistant: Bien sûr! The Arc de Triomphe is one of Paris' most iconic landmarks, and it's located at the center of the famous Champs-Élysées avenue. Specifically, it's situated at the western end of the avenue, at the Place Charles de Gaulle (formerly known as Place de l'Étoile).\n",
      "\n",
      "The Arc de Triomphe honors the soldiers who fought and died for France, particularly during the Napoleonic Wars. It's a grand monument that offers breathtaking \n",
      "\n",
      "User: What are the must-see artworks ate the Louvre Museum?\n",
      "Assistant: Le Louvre! As one of the world's greatest museums, it's home to an incredible collection of art and artifacts from around the world. Here are some of the must-see artworks to add to your itinerary:\n",
      "\n",
      "1. **Mona Lisa** by Leonardo da Vinci (1503-1506): The enigmatic smile of the world's most famous painting is a must-see. Be prepared for crowds, but it's worth the wait.\n",
      "2. **Venus de Milo** ( \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exercise:\n",
    "\n",
    "# Define the model to use\n",
    "model = \"llama-3.1-8b-instant\"\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a virtual Parisian expert, delivering valuable insights into the \n",
    "city's iconic landmarks and hidden treasures. You will respond intelligently to a \n",
    "set of common questions, providing a more engaging and immersive travel planning\n",
    " experience for the clientele of Peterman Reality Tours.\n",
    "\"\"\"\n",
    "\n",
    "conversation = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "\n",
    "questions = [\n",
    "    \"How far away is the Louvvre from the Eiffel Tower (in miles) if you are driving?\",\n",
    "    \"Where is the Arc de Triomphe?\",\n",
    "    \"What are the must-see artworks ate the Louvre Museum?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(\"User:\", q)\n",
    "    \n",
    "    conversation.append({\"role\": \"user\", \"content\": q})\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=conversation,\n",
    "        temperature=0.0,\n",
    "        max_tokens=100\n",
    "    )\n",
    "    \n",
    "    conversation.append({\"role\": \"assistant\", \"content\": response.choices[0].message.content})\n",
    "    print(\"Assistant:\", response.choices[0].message.content, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
